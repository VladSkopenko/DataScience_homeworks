{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDQsVp1MWncQ",
        "outputId": "48da975a-352e-4f04-e769-fafbed50bdda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Descent - w: [4766729.23596627  821968.67551979  300259.83239246  696447.05416246]\n",
            "Analytical Solution - Optimal w: [4766729.24770642  821968.58935343  300259.16468032  696447.75898579]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def h(X, w):\n",
        "    return np.dot(X, w)\n",
        "def loss_function(X, y, w):\n",
        "    return np.square(h(X, w) - y).sum() / (2 * len(X))\n",
        "\n",
        "def gradient_step(X, y, w, learning_rate):\n",
        "    m = len(y)\n",
        "    grad = (X.T @ (h(X, w) - y)) / m\n",
        "    w -= learning_rate * grad\n",
        "    return w\n",
        "def gradient(X, y, learning_rate, num_iter, eps):\n",
        "    ones = np.ones((X.shape[0], 1))\n",
        "    X = np.hstack((ones, X))\n",
        "\n",
        "    w = np.zeros(X.shape[1])\n",
        "\n",
        "    loss = loss_function(X, y, w)\n",
        "    loss_history = [loss]\n",
        "\n",
        "    for _ in range(num_iter):\n",
        "        w = gradient_step(X, y, w, learning_rate)\n",
        "\n",
        "        loss = loss_function(X, y, w)\n",
        "        if abs(loss - loss_history[-1]) < eps:\n",
        "            loss_history.append(loss)\n",
        "            break\n",
        "\n",
        "        loss_history.append(loss)\n",
        "\n",
        "    return w, loss_history\n",
        "\n",
        "df = pd.read_csv(\"Housing.csv\")\n",
        "norm = df.copy()\n",
        "columns = ['price', 'area', 'bedrooms', 'bathrooms']\n",
        "for column in columns[1:]:\n",
        "    norm[column] = (df[column] - df[column].mean()) / df[column].std()\n",
        "\n",
        "\n",
        "X = norm[['area', 'bedrooms', 'bathrooms']].values\n",
        "y = norm['price'].values\n",
        "\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_iter = 100000\n",
        "eps = 1e-12\n",
        "\n",
        "\n",
        "w_gradient, loss_history = gradient(X, y, learning_rate, num_iter, eps)\n",
        "print('Gradient Descent - w:', w_gradient)\n",
        "\n",
        "ones = np.ones((X.shape[0], 1))\n",
        "X = np.hstack((ones, X))\n",
        "w_analytical = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
        "print('Analytical Solution - Optimal w:', w_analytical)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Як можна побачити з результатів вищенаведених розрахунків, значення параметрів\n",
        " отримані за допомогою scikit-learn повністю співпадають зі значеннями цих же параметрів отриманих в результаті аналітичного рішення."
      ],
      "metadata": {
        "id": "WujwU_SdXbh8"
      }
    }
  ]
}